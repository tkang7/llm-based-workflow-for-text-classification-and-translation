{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, AgentType, Tool\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.tools.wikipedia.tool import WikipediaQueryRun\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "class ResearchAgent:\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "        \n",
    "        # Define tools\n",
    "        self.tools = [\n",
    "            Tool(\n",
    "                name=\"Wikipedia\",\n",
    "                func=self.search_wikipedia,\n",
    "                description=\"Useful for searching Wikipedia articles\"\n",
    "            ),\n",
    "            Tool(\n",
    "                name=\"Calculator\",\n",
    "                func=self.calculate,\n",
    "                description=\"Useful for performing mathematical calculations\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Initialize agent\n",
    "        self.agent = initialize_agent(\n",
    "            tools=self.tools,\n",
    "            llm=self.llm,\n",
    "            agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "            memory=self.memory,\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    def search_wikipedia(self, query):\n",
    "        # Initialize Wikipedia wrapper and query tool\n",
    "        wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "        return wikipedia.run(query)\n",
    "    \n",
    "    def calculate(self, expression):\n",
    "        try:\n",
    "            # This is dangerous; replace with safer parsing for production\n",
    "            result = eval(expression)\n",
    "            return str(result)\n",
    "        except Exception as e:\n",
    "            return f\"Error in calculation: {str(e)}\"\n",
    "    \n",
    "    def run(self, query):\n",
    "        return self.agent.run(query)\n",
    "\n",
    "\n",
    "# ---------- Usage Example ----------\n",
    "# Load your model\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Hugging Face pipeline\n",
    "hf_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() or torch.backends.mps.is_available() else -1,\n",
    "    max_length=512,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in HuggingFacePipeline for LangChain\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
    "\n",
    "# Initialize ResearchAgent\n",
    "research_agent = ResearchAgent(llm=llm)\n",
    "\n",
    "# Run a query\n",
    "result = research_agent.run(\n",
    "    \"Tell me about artificial intelligence and calculate how many years since it was first coined as a term in 1956.\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/tkang/Documents/Projects/MDS/llm-based-workflow-for-text-classification-and-translation')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import HuggingFacePipeline\n",
    "\n",
    "from src.milestone2.models.llm import LLM\n",
    "from src.milestone2.models.agent import Agent\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# llm = LLM().create(\"ollama-llama3.2\")\n",
    "llm = LLM().create(\"medium\")\n",
    "# wrap it in hugging face pipeline\n",
    "llm = HuggingFacePipeline(pipeline=llm)\n",
    "\n",
    "agent = Agent(llm)\n",
    "result = agent.run({\"input\": \"Ugh I'm so frustrated\"})\n",
    "print(result)\n",
    "\n",
    "# sent_subagent = SentimentAnalysisSubAgent()\n",
    "# res = sent_subagent.analyze(llm, \"I am happy\")\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentiment analysis\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "class SentimentAnalysisSubAgent:\n",
    "    def __init__(self):\n",
    "        model_id = \"ibm-granite/granite-3.0-2b-instruct\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "        # Use MPS (Apple Silicon)\n",
    "        self.device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "        self.model = self.model.to(self.device)\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            \"text-generation\",  # <-- Use \"text-generation\" for CausalLM models\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            return_full_text=False,\n",
    "            max_length=512,\n",
    "            temperature=0,\n",
    "            device=-1  # Explicitly use CPU/MPS for pipeline (Hugging Face currently doesn't fully support MPS in `pipeline`)\n",
    "        )\n",
    "\n",
    "        self.prompt_description = \"\"\"\n",
    "        You are a sentiment analysis expert. Analyze the following sentence and return:\n",
    "        1. The overall sentiment: Positive, Negative, Neutral, or Mixed.\n",
    "        2. An explanation describing why it was classified that way.\n",
    "        \"\"\"\n",
    "\n",
    "        self.prompt_output_format = \"\"\"\n",
    "        Response Format (JSON):\n",
    "        {\n",
    "            \"sentiment_label\": \"...\",\n",
    "            \"explanation\": \"...\"\n",
    "        }\n",
    "        \"\"\"\n",
    "\n",
    "    def analyze(self, question):\n",
    "        full_prompt = (\n",
    "            self.prompt_description +\n",
    "            f\"\\nSentence: {question}\\n\" +\n",
    "            self.prompt_output_format\n",
    "        )\n",
    "\n",
    "        result = self.generator(full_prompt, max_length=512, do_sample=False)\n",
    "\n",
    "        # If you want to free memory after one inference\n",
    "        # self.free_mps_memory()\n",
    "\n",
    "        return result\n",
    "    \n",
    "    # def free_mps_memory(self):\n",
    "    #     del self.model\n",
    "    #     del self.tokenizer\n",
    "    #     gc.collect()\n",
    "\n",
    "    #     # Only if on MPS and using PyTorch >= 2.0\n",
    "    #     if torch.backends.mps.is_available() and hasattr(torch.mps, \"empty_cache\"):\n",
    "    #         torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_agent = SentimentAnalysisSubAgent()\n",
    "\n",
    "res = sentiment_analysis_agent.analyze(\"I love this product!\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a14da795b9d848a79a940977b657132a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Load DetoxLLM-7B model and tokenizer from HuggingFace\n",
    "model_name = \"UBC-NLP/DetoxLLM-7B\"\n",
    "\n",
    "# If you have a GPU, use device_map=\"auto\". For CPU-only, add torch_dtype=torch.float32\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "detoxify_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_k=50,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# Detoxify function\n",
    "def detoxify_sentence(text: str) -> str:\n",
    "    prompt = (\n",
    "        \"### Instruction:\\n\"\n",
    "        \"Detoxify the following sentence while keeping its meaning intact.\\n\\n\"\n",
    "        f\"### Input:\\n{text}\\n\\n### Response:\"\n",
    "    )\n",
    "    response = detoxify_pipeline(prompt, num_return_sequences=1)\n",
    "    detoxified_text = response[0]['generated_text'].split(\"### Response:\")[-1].strip()\n",
    "    return detoxified_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_28356/2937384922.py:87: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n",
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_28356/2937384922.py:118: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThe sentence is already in English, so no translation is needed. I need to analyze its sentiment and toxicity, and then detoxify it.\n",
      "Action: SentimentAnalysisWithReason\n",
      "Action Input: 'You're so stupid and annoying! I can't stand you.'\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8b/lb1cxfqj6zx5dtq789h2k8xr0000gn/T/ipykernel_28356/2937384922.py:36: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = agent_llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[33;1m\u001b[1;3mThe sentiment of the sentence is Negative. The reasoning behind this is that the speaker is using derogatory language such as \"stupid\" and \"annoying\" to describe the person they are speaking to. Additionally, the phrase \"I can't stand you\" is a strong expression of dislike or irritation. All these elements contribute to a negative sentiment.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mNow that I have analyzed the sentiment of the sentence, I need to analyze its toxicity.\n",
      "Action: ToxicityAnalysisWithReason\n",
      "Action Input: 'You're so stupid and annoying! I can't stand you.'\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mThe sentence is toxic. It contains direct insults and negative language aimed at another person, which is a form of verbal abuse. The speaker is expressing their dislike and intolerance towards the person they are addressing in a disrespectful and harmful manner. This kind of communication can be damaging to the recipient's mental and emotional well-being.\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mNow that I have analyzed the toxicity of the sentence, I need to detoxify it.\n",
      "Action: DetoxifySentence\n",
      "Action Input: 'You're so stupid and annoying! I can't stand you.'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mYou're not very intelligent and can be bothersome. I find you quite dislikable.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: \n",
      "[\n",
      "    {\n",
      "        \"sentiment_label\": \"Negative\",\n",
      "        \"explanation\": \"The sentiment of the sentence is Negative. The reasoning behind this is that the speaker is using derogatory language such as 'stupid' and 'annoying' to describe the person they are speaking to. Additionally, the phrase 'I can't stand you' is a strong expression of dislike or irritation. All these elements contribute to a negative sentiment.\"\n",
      "    },\n",
      "    {\n",
      "        \"toxicity_label\": \"Toxic\",\n",
      "        \"explanation\": \"The sentence is toxic. It contains direct insults and negative language aimed at another person, which is a form of verbal abuse. The speaker is expressing their dislike and intolerance towards the person they are addressing in a disrespectful and harmful manner. This kind of communication can be damaging to the recipient's mental and emotional well-being.\"\n",
      "    },\n",
      "    {\n",
      "        \"detoxified_text\": \"You're not very intelligent and can be bothersome. I find you quite dislikable.\",\n",
      "        \"explanation\": \"The sentence has been detoxified by replacing the direct insults with less harsh language. The phrase 'You're so stupid' has been changed to 'You're not very intelligent', and 'annoying' has been changed to 'bothersome'. The phrase 'I can't stand you' has been softened to 'I find you quite dislikable'. These changes maintain the original sentiment of the sentence while reducing its toxicity.\"\n",
      "    }\n",
      "]\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Agent Response:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"sentiment_label\": \"Negative\",\n",
      "        \"explanation\": \"The sentiment of the sentence is Negative. The reasoning behind this is that the speaker is using derogatory language such as 'stupid' and 'annoying' to describe the person they are speaking to. Additionally, the phrase 'I can't stand you' is a strong expression of dislike or irritation. All these elements contribute to a negative sentiment.\"\n",
      "    },\n",
      "    {\n",
      "        \"toxicity_label\": \"Toxic\",\n",
      "        \"explanation\": \"The sentence is toxic. It contains direct insults and negative language aimed at another person, which is a form of verbal abuse. The speaker is expressing their dislike and intolerance towards the person they are addressing in a disrespectful and harmful manner. This kind of communication can be damaging to the recipient's mental and emotional well-being.\"\n",
      "    },\n",
      "    {\n",
      "        \"detoxified_text\": \"You're not very intelligent and can be bothersome. I find you quite dislikable.\",\n",
      "        \"explanation\": \"The sentence has been detoxified by replacing the direct insults with less harsh language. The phrase 'You're so stupid' has been changed to 'You're not very intelligent', and 'annoying' has been changed to 'bothersome'. The phrase 'I can't stand you' has been softened to 'I find you quite dislikable'. These changes maintain the original sentiment of the sentence while reducing its toxicity.\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.tools import Tool\n",
    "from textblob import TextBlob\n",
    "from detoxify import Detoxify\n",
    "import openai\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=\"auto\")\n",
    "# generation_pipeline = pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     max_length=512,\n",
    "#     temperature=0.1,\n",
    "#     # top_p=0.95,\n",
    "#     # repetition_penalty=1.2\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "gpt_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "\n",
    "def sentiment_analysis_with_reason(text: str) -> str:\n",
    "    prompt = (\n",
    "        f\"What is the sentiment (Positive, Negative, Neutral) of the following sentence? \"\n",
    "        f\"Explain your reasoning in detail.\\n\\n\"\n",
    "        f\"Sentence: \\\"{text}\\\"\"\n",
    "    )\n",
    "    \n",
    "    # This uses the agent's LLM to answer the question (chat interface)\n",
    "    agent_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "    response = agent_llm.predict(prompt)\n",
    "    return response\n",
    "\n",
    "# --- Toxicity Analysis with Reasoning Tool ---\n",
    "def toxicity_analysis_with_reason(text: str) -> str:\n",
    "    prompt = (\n",
    "        f\"Is the following sentence toxic or non-toxic? \"\n",
    "        f\"Explain your reasoning in detail.\\n\\n\"\n",
    "        f\"Sentence: \\\"{text}\\\"\"\n",
    "    )\n",
    "    \n",
    "    # This uses the agent's LLM to answer the question (chat interface)\n",
    "    agent_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "    response = agent_llm.predict(prompt)\n",
    "    return response\n",
    "\n",
    "# --- Translate to English Tool ---\n",
    "def translate_to_english(text: str) -> str:\n",
    "    prompt = f\"Translate the following sentence into English:\\n\\n{text}\"\n",
    "    \n",
    "    # This uses the agent's LLM to answer the question (chat interface)\n",
    "    agent_llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)\n",
    "    response = agent_llm.predict(prompt)\n",
    "    return response\n",
    "\n",
    "# --- Create Tools ---\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=translate_to_english,\n",
    "        name=\"TranslateToEnglish\",\n",
    "        description=\"Use this tool to translate sentences into English.\"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=sentiment_analysis_with_reason,\n",
    "        name=\"SentimentAnalysisWithReason\",\n",
    "        description=\"Use this tool to analyze sentiment of a sentence. Returns sentiment label and reasoning.\"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=toxicity_analysis_with_reason,\n",
    "        name=\"ToxicityAnalysisWithReason\",\n",
    "        description=\"Use this tool to analyze toxicity of a sentence. Returns toxicity label and reasoning.\"\n",
    "    ),\n",
    "    Tool.from_function(\n",
    "        func=detoxify_sentence,\n",
    "        name=\"DetoxifySentence\",\n",
    "        description=\"Use this tool to detoxify a toxic sentence and rewrite it without harmful language.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Initialize LangChain Agent ---\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=gpt_llm,\n",
    "    # llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = (\n",
    "        \"Here's a sentence: 'You're so stupid and annoying! I can't stand you.'\\n\"\n",
    "        \"Please translate it to English if necessary, analyze its sentiment with reasons, \"\n",
    "        \"analyze its toxicity with reasons, and detoxify it.\\n\"\n",
    "        \"Return the result strictly as a JSON array in this format:\\n\\n\"\n",
    "        \"[\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"        \\\"sentiment_label\\\": \\\"<Sentiment Label>\\\",\\n\"\n",
    "        \"        \\\"explanation\\\": \\\"<Explain why the sentiment was classified as such>\\\"\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"        \\\"toxicity_label\\\": \\\"<Toxic/Non-Toxic>\\\",\\n\"\n",
    "        \"        \\\"explanation\\\": \\\"<Explain why the sentence was considered toxic or not>\\\"\\n\"\n",
    "        \"    },\\n\"\n",
    "        \"    {\\n\"\n",
    "        \"        \\\"detoxified_text\\\": \\\"<Detoxified Sentence>\\\",\\n\"\n",
    "        \"        \\\"explanation\\\": \\\"<Explain how you modified the sentence to make it non-toxic>\\\"\\n\"\n",
    "        \"    }\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    response = agent.run(query)\n",
    "\n",
    "    print(\"\\nAgent Response:\\n\")\n",
    "    print(response)\n",
    "\n",
    "    # while True:\n",
    "    #     query = input(\"\\nAsk the agent to perform a task (or type 'exit'): \")\n",
    "    #     if query.lower() == \"exit\":\n",
    "    #         break\n",
    "    #     response = agent.run(query)\n",
    "    #     print(f\"\\nAgent Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
